{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2DfeB/mKczIxhrT+wX4Bk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KuzmenkoO/amazinum_home_work/blob/main/lesson_33_nltk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K3NVbvgaF4Q",
        "outputId": "e540094e-1ee0-4463-e8ee-a40f771163b6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('words')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAFm5jhqaRqf",
        "outputId": "5e7389bc-1dec-4298-8191-5f45b3c33687"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg, words\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from nltk.metrics.distance import edit_distance\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# Завантажуємо текст \"Мобі Дік\"\n",
        "moby_raw = gutenberg.raw('melville-moby_dick.txt')\n",
        "tokens = word_tokenize(moby_raw)\n",
        "unique_tokens = set(tokens)\n",
        "\n",
        "# Example 1: Кількість токенів (слів + пунктуації)\n",
        "def example_one():\n",
        "    return len(tokens)\n",
        "\n",
        "# Example 2: Кількість унікальних токенів\n",
        "def example_two():\n",
        "    return len(unique_tokens)\n",
        "\n",
        "# Example 3: Кількість унікальних лематизованих токенів (як дієслів)\n",
        "def example_three():\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized = [lemmatizer.lemmatize(w, 'v') for w in tokens]\n",
        "    return len(set(lemmatized))\n",
        "\n",
        "# Question 1: Лексичне різноманіття (відношення унікальних до загальної кількості)\n",
        "def answer_one():\n",
        "    return len(unique_tokens) / len(tokens)\n",
        "\n",
        "# Question 2: Відсоток вживання слова 'whale' або 'Whale'\n",
        "def answer_two():\n",
        "    whale_count = sum(1 for w in tokens if w.lower() == 'whale')\n",
        "    return 100 * whale_count / len(tokens)\n",
        "\n",
        "# Question 3: Топ-20 найчастіших токенів\n",
        "def answer_three():\n",
        "    freq_dist = Counter(tokens)\n",
        "    return freq_dist.most_common(20)\n",
        "\n",
        "# Question 4: Токени довші за 5 символів, що з’являються >150 разів\n",
        "def answer_four():\n",
        "    freq_dist = Counter(tokens)\n",
        "    filtered = [word for word in freq_dist if len(word) > 5 and freq_dist[word] > 150]\n",
        "    return sorted(filtered)\n",
        "\n",
        "# Question 5: Найдовше слово та його довжина\n",
        "def answer_five():\n",
        "    words_only = [w for w in tokens if w.isalpha() or '-' in w]\n",
        "    longest = max(words_only, key=len)\n",
        "    return (longest, len(longest))\n",
        "\n",
        "# Question 6: Унікальні слова, що з’являються > 2000 разів\n",
        "def answer_six():\n",
        "    freq_dist = Counter(tokens)\n",
        "    filtered = [(count, word) for word, count in freq_dist.items() if count > 2000]\n",
        "    return sorted(filtered, reverse=True)\n",
        "\n",
        "# Question 7: Середня кількість токенів у реченні\n",
        "def answer_seven():\n",
        "    sentences = sent_tokenize(moby_raw)\n",
        "    tokens_per_sentence = [len(word_tokenize(sent)) for sent in sentences]\n",
        "    return np.mean(tokens_per_sentence)\n",
        "\n",
        "# Question 8: Топ-5 частин мови\n",
        "def answer_eight():\n",
        "    tagged = pos_tag(tokens)\n",
        "    tag_freq = Counter(tag for word, tag in tagged)\n",
        "    return tag_freq.most_common(5)\n",
        "\n",
        "# Question 9: Рекомендації правильного написання на основі edit distance\n",
        "def answer_nine(default_words=['cormulent', 'incendenece', 'validrate']):\n",
        "    correct_spellings = words.words()\n",
        "    results = []\n",
        "    for word in default_words:\n",
        "        candidates = [w for w in correct_spellings if w.startswith(word[0])]\n",
        "        closest = min(candidates, key=lambda x: edit_distance(word, x, transpositions=True))\n",
        "        results.append(closest)\n",
        "    return results\n",
        "\n",
        "# ===== Перевірка результатів (можна закоментувати при імпорті модуля) =====\n",
        "print(\"Example 1:\", example_one())\n",
        "print(\"Example 2:\", example_two())\n",
        "print(\"Example 3:\", example_three())\n",
        "print(\"Answer 1:\", answer_one())\n",
        "print(\"Answer 2:\", answer_two())\n",
        "print(\"Answer 3:\", answer_three()[:10])\n",
        "print(\"Answer 4:\", answer_four())\n",
        "print(\"Answer 5:\", answer_five())\n",
        "print(\"Answer 6:\", answer_six())\n",
        "print(\"Answer 7:\", answer_seven())\n",
        "print(\"Answer 8:\", answer_eight())\n",
        "print(\"Answer 9:\", answer_nine())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFWSseCWaX-Y",
        "outputId": "120de885-6e73-4feb-b5b4-3bb9731566b5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1: 255222\n",
            "Example 2: 20639\n",
            "Example 3: 16908\n",
            "Answer 1: 0.08086685317096488\n",
            "Answer 2: 0.429038249053765\n",
            "Answer 3: [(',', 19204), ('the', 13715), ('.', 7306), ('of', 6513), ('and', 6010), ('a', 4545), ('to', 4515), (';', 4173), ('in', 3908), ('that', 2981)]\n",
            "Answer 4: ['Captain', 'Pequod', 'Queequeg', 'Starbuck', 'almost', 'before', 'himself', 'little', 'seemed', 'should', 'though', 'through', 'whales', 'without']\n",
            "Answer 5: (\"twelve-o'clock-at-night\", 23)\n",
            "Answer 6: [(19204, ','), (13715, 'the'), (7306, '.'), (6513, 'of'), (6010, 'and'), (4545, 'a'), (4515, 'to'), (4173, ';'), (3908, 'in'), (2981, 'that'), (2459, 'his'), (2206, 'it'), (2121, 'I')]\n",
            "Answer 7: 25.90560292326431\n",
            "Answer 8: [('NN', 32722), ('IN', 28659), ('DT', 25885), (',', 19204), ('JJ', 17598)]\n",
            "Answer 9: ['corpulent', 'intendence', 'validate']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import gutenberg, words\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from nltk.metrics.distance import edit_distance\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Завантаження всіх необхідних ресурсів\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('words')\n",
        "\n",
        "# Отримуємо необроблений текст \"Moby Dick\"\n",
        "moby_raw = gutenberg.raw('melville-moby_dick.txt')\n",
        "\n",
        "# Example 1: Кількість усіх токенів у тексті (слова + пунктуація)\n",
        "def example_one():\n",
        "    moby_tokens = word_tokenize(moby_raw)\n",
        "    return len(moby_tokens)\n",
        "\n",
        "# Example 2: Кількість унікальних токенів у тексті\n",
        "def example_two():\n",
        "    moby_tokens = word_tokenize(moby_raw)\n",
        "    return len(set(moby_tokens))\n",
        "\n",
        "# Example 3: Кількість унікальних лематизованих дієслів\n",
        "def example_three():\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    moby_tokens = word_tokenize(moby_raw)\n",
        "    lemmatized = [lemmatizer.lemmatize(w, 'v') for w in moby_tokens]\n",
        "    return len(set(lemmatized))\n",
        "\n",
        "# Question 1: Лексичне різноманіття (унікальні токени / загальна кількість токенів)\n",
        "def answer_one():\n",
        "    moby_tokens = word_tokenize(moby_raw)\n",
        "    return len(set(moby_tokens)) / len(moby_tokens)\n",
        "\n",
        "# Question 2: Частка слова \"whale\" або \"Whale\" серед усіх токенів\n",
        "def answer_two():\n",
        "    moby_tokens = word_tokenize(moby_raw)\n",
        "    whale_count = sum(1 for w in moby_tokens if w.lower() == 'whale')\n",
        "    return whale_count / len(moby_tokens)\n",
        "\n",
        "# Question 3: 20 найчастіших токенів (включаючи пунктуацію)\n",
        "def answer_three():\n",
        "    moby_tokens = word_tokenize(moby_raw)\n",
        "    freq_dist = Counter(moby_tokens)\n",
        "    return freq_dist.most_common(20)\n",
        "\n",
        "# Question 4: Токени довші за 5 символів і з частотою > 150, відсортовані\n",
        "def answer_four():\n",
        "    moby_tokens = word_tokenize(moby_raw)\n",
        "    freq_dist = Counter(moby_tokens)\n",
        "    filtered = [word for word in freq_dist if len(word) > 5 and freq_dist[word] > 150]\n",
        "    return sorted(filtered)\n",
        "\n",
        "# Question 5: Найдовше слово в тексті та його довжина\n",
        "def answer_five():\n",
        "    moby_tokens = word_tokenize(moby_raw)\n",
        "    words_only = [w for w in moby_tokens if w.isalpha() or '-' in w]\n",
        "    longest = max(words_only, key=len)\n",
        "    return (longest, len(longest))\n",
        "\n",
        "# Question 6: Частотність слів (не пунктуації), які зустрічаються > 2000 разів\n",
        "def answer_six():\n",
        "    moby_tokens = word_tokenize(moby_raw)\n",
        "    freq_dist = Counter(w for w in moby_tokens if w.isalpha())\n",
        "    return sorted([(v, k) for k, v in freq_dist.items() if v > 2000], reverse=True)\n",
        "\n",
        "# Question 7: Середня кількість токенів на речення\n",
        "def answer_seven():\n",
        "    sentences = sent_tokenize(moby_raw)\n",
        "    tokens_per_sentence = [len(word_tokenize(sent)) for sent in sentences]\n",
        "    return np.mean(tokens_per_sentence)\n",
        "\n",
        "# Question 8: Топ-5 найчастіших частин мови (POS)\n",
        "def answer_eight():\n",
        "    moby_tokens = word_tokenize(moby_raw)\n",
        "    tagged = pos_tag(moby_tokens)\n",
        "    tag_freq = Counter(tag for word, tag in tagged)\n",
        "    return tag_freq.most_common(5)\n",
        "\n",
        "# Question 9: Рекомендація правопису (edit distance + спільна перша літера)\n",
        "def answer_nine(default_words=['cormulent', 'incendenece', 'validrate']):\n",
        "    correct_spellings = words.words()\n",
        "    results = []\n",
        "    for word in default_words:\n",
        "        candidates = [w for w in correct_spellings if w.startswith(word[0])]\n",
        "        closest = min(candidates, key=lambda x: edit_distance(word, x, transpositions=True))\n",
        "        results.append(closest)\n",
        "    return results\n",
        "\n",
        "# ========== ТЕСТИ (можеш запускати для перевірки) ==========\n",
        "print(\"Example 1:\", example_one())\n",
        "print(\"Example 2:\", example_two())\n",
        "print(\"Example 3:\", example_three())\n",
        "print(\"Answer 1:\", answer_one())\n",
        "print(\"Answer 2:\", answer_two())\n",
        "print(\"Answer 3:\", answer_three()[:10])\n",
        "print(\"Answer 4:\", answer_four())\n",
        "print(\"Answer 5:\", answer_five())\n",
        "print(\"Answer 6:\", answer_six())\n",
        "print(\"Answer 7:\", answer_seven())\n",
        "print(\"Answer 8:\", answer_eight())\n",
        "print(\"Answer 9:\", answer_nine())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jx2n0fbRdl9R",
        "outputId": "2ab0475d-338e-45c5-d574-8026017e5bc2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1: 255222\n",
            "Example 2: 20639\n",
            "Example 3: 16908\n",
            "Answer 1: 0.08086685317096488\n",
            "Answer 2: 0.004290382490537649\n",
            "Answer 3: [(',', 19204), ('the', 13715), ('.', 7306), ('of', 6513), ('and', 6010), ('a', 4545), ('to', 4515), (';', 4173), ('in', 3908), ('that', 2981)]\n",
            "Answer 4: ['Captain', 'Pequod', 'Queequeg', 'Starbuck', 'almost', 'before', 'himself', 'little', 'seemed', 'should', 'though', 'through', 'whales', 'without']\n",
            "Answer 5: (\"twelve-o'clock-at-night\", 23)\n",
            "Answer 6: [(13715, 'the'), (6513, 'of'), (6010, 'and'), (4545, 'a'), (4515, 'to'), (3908, 'in'), (2981, 'that'), (2459, 'his'), (2206, 'it'), (2121, 'I')]\n",
            "Answer 7: 25.90560292326431\n",
            "Answer 8: [('NN', 32722), ('IN', 28659), ('DT', 25885), (',', 19204), ('JJ', 17598)]\n",
            "Answer 9: ['corpulent', 'intendence', 'validate']\n"
          ]
        }
      ]
    }
  ]
}